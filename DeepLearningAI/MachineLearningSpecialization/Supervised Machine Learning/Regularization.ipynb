{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867eceff-f9a8-44e0-93d8-20ca50fe3428",
   "metadata": {},
   "source": [
    "## Cost function for regularized linear regression\n",
    "\n",
    "有正則化的線性回歸成本公式是:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 $$ \n",
    "其中:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  $$ \n",
    "\n",
    "與一般線性回歸成本的差異是多了正則化項目,  <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "    \n",
    "注意，b並不需要做正則化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69f8d89-c2f6-4d20-9501-6ff65e52cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a0d76b-456b-4896-a522-cd9eb9574770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return np.matmul(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706b182f-6d6a-46e5-91ca-b7c186ecb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, w, b, lambda_):\n",
    "    pred = model(x, w, b)\n",
    "    cost = np.mean((pred - y) ** 2) / 2 + (lambda_ / (2 * len(x))) * np.sum(w ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4513cb6-8bd5-44b0-a1fa-f3061547fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.rand(5, 6)\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "w = np.random.rand(x.shape[1]).reshape(-1, ) - 0.5\n",
    "b = 0.5\n",
    "lambda_ = 0.7\n",
    "\n",
    "cost = computeCost(x, y, w, b, lambda_)\n",
    "print(\"Regularized cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98166710-3048-479f-91e8-128bfc617f8a",
   "metadata": {},
   "source": [
    "### Computing the Gradient with regularization (both linear/logistic)\n",
    "對於正則化來說，不管是線性回歸函是邏輯回歸都是一樣的，唯一差別的是計算 $f_{\\mathbf{w}b}$.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "      \n",
    "* 對於  <span style=\"color:blue\"> **linear** </span> regression 模型  \n",
    "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "* 對於  <span style=\"color:blue\"> **logistic** </span> regression 模型  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    其中 $g(z)$ 是sigmoid:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n",
    "正則化只新增 <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be91f2b2-bccc-40b8-aef0-2575081e2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(x, y, w, b, lambda_):\n",
    "    pred = model(x, w, b)\n",
    "    err = pred - y\n",
    "    err = np.expand_dims(err, axis = 1)\n",
    "    gradW = np.mean(err * x, axis = 0) + lambda_ / len(x) * w\n",
    "    gradB = np.mean(err)\n",
    "    return gradW, gradB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5305bc9-22c8-4136-95fc-10bc369b6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradB: 0.6648774569425726\n",
      "Regularized gardW:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.rand(5, 3)\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "w = np.random.rand(x.shape[1])\n",
    "b = 0.5\n",
    "lambda_ = 0.7\n",
    "gradW, gradB =  computeGradient(x, y, w, b, lambda_)\n",
    "\n",
    "print(f\"gradB: {gradB}\", )\n",
    "print(f\"Regularized gardW:\\n {gradW.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716a62d-89aa-4974-a236-0981a80ba177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
